"""
ðŸ§  CREATOR INTEL V2 DEEP-DIVE AUDIT
====================================

This test suite audits the Creator Intel V2 data pipeline for:
1. Quota Management Resilience - Budget exhaustion, circuit breaker behavior
2. Collector Silent Failures - Partial batch failures, retry exhaustion
3. Analyzer Error Isolation - One analyzer failure affecting others
4. Aggregation Data Integrity - Redis â†’ PostgreSQL sync issues
5. Orchestrator Coordination - Task scheduling, graceful shutdown

Run with: python3 -m pytest backend/tests/integration/test_intel_v2_audit.py -v

CRITICAL FINDINGS SUMMARY (updated as tests reveal issues):
- See TestAuditSummary at the bottom for comprehensive findings
"""

import asyncio
import json
import logging
import time
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta, date
from typing import Any, Dict, List, Optional
from unittest.mock import AsyncMock, MagicMock, patch
import uuid

import pytest

logger = logging.getLogger(__name__)


# =============================================================================
# SECTION 1: QUOTA MANAGEMENT RESILIENCE TESTS
# =============================================================================

class TestQuotaManagementResilience:
    """Tests for quota manager behavior under stress and failure conditions."""
    
    @pytest.fixture
    def mock_redis(self):
        """Create a mock async Redis client."""
        redis = AsyncMock()
        redis.get.return_value = None
        redis.set.return_value = True
        redis.incr.return_value = 1
        redis.expire.return_value = True
        return redis

    @pytest.mark.asyncio
    async def test_quota_exhaustion_mid_batch(self, mock_redis):
        """
        AUDIT: When quota exhausts mid-batch, remaining games are skipped silently.
        
        RISK: Users may see incomplete data without knowing quota was exhausted.
        """
        from backend.services.intel.collectors.quota_manager import QuotaManager, QuotaBucket
        
        manager = QuotaManager(mock_redis)
        
        # Set up quota bucket with very limited quota (only 1 unit remaining)
        manager._quota_bucket = QuotaBucket(
            window_start=datetime.now(timezone.utc),
            units_used=9999,  # Only 1 unit remaining - not enough for any game
            units_limit=10000,
        )
        manager._initialized = True
        
        # Set up priorities for multiple games - each needs ~2 units (trending + details)
        from backend.services.intel.collectors.quota_manager import CollectionPriority
        manager._priorities = {
            "fortnite": CollectionPriority("fortnite", 1, 0.1, 50),  # Needs ~2 units
            "valorant": CollectionPriority("valorant", 1, 0.1, 50),  # Needs ~2 units
            "minecraft": CollectionPriority("minecraft", 1, 0.1, 50),  # Needs ~2 units
        }
        
        # Get collection schedule
        schedule = manager.get_collection_schedule()
        
        # AUDIT FINDING: Games are skipped when quota is insufficient
        # But there's no indication to callers about WHY games were skipped
        assert len(schedule) == 0, "All games should be skipped due to insufficient quota"
        
        logger.warning(
            "AUDIT: All 3 games skipped due to quota exhaustion. "
            "get_collection_schedule() returns empty list but no indication of WHY. "
            "RECOMMENDATION: Return (schedule, skipped_reasons) tuple."
        )
    
    @pytest.mark.asyncio
    async def test_circuit_breaker_opens_after_failures(self, mock_redis):
        """
        AUDIT: Circuit breaker opens after consecutive failures.
        Verify it actually blocks subsequent operations.
        """
        from backend.services.intel.collectors.quota_manager import QuotaManager, QuotaBucket
        from backend.services.intel.core.exceptions import IntelCircuitOpenError
        
        manager = QuotaManager(mock_redis)
        manager._quota_bucket = QuotaBucket(
            window_start=datetime.now(timezone.utc),
            units_used=0,
        )
        manager._initialized = True
        
        # Simulate 3 consecutive failures
        for i in range(3):
            mock_redis.incr.return_value = i + 1
            await manager.record_failure("fortnite", Exception("API error"))
        
        # Circuit should be open
        assert manager._circuit_open is True
        assert manager._circuit_open_until is not None
        
        # Verify check_quota raises when circuit is open
        with pytest.raises(IntelCircuitOpenError) as exc_info:
            manager.check_quota(1)
        
        assert "Circuit breaker is open" in exc_info.value.message
        
        logger.info(
            "AUDIT: Circuit breaker correctly opens after 3 failures. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_quota_reset_at_midnight_pt(self, mock_redis):
        """
        AUDIT: Quota should reset at midnight Pacific Time.
        Test timezone handling edge cases.
        """
        from backend.services.intel.collectors.quota_manager import QuotaManager
        
        manager = QuotaManager(mock_redis)
        
        # Create a bucket from "yesterday" in PT
        try:
            from zoneinfo import ZoneInfo
            pt = ZoneInfo("America/Los_Angeles")
        except ImportError:
            pt = timezone(timedelta(hours=-8))
        
        yesterday_pt = datetime.now(pt) - timedelta(days=1)
        yesterday_utc = yesterday_pt.astimezone(timezone.utc)
        
        # Test that _is_new_quota_day correctly identifies day change
        is_new_day = manager._is_new_quota_day(yesterday_utc)
        
        assert is_new_day is True, "Should detect new quota day"
        
        logger.info(
            "AUDIT: Quota reset timezone handling works correctly. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_quota_persistence_failure_silent(self, mock_redis):
        """
        AUDIT: If Redis fails during quota persistence, the failure is now handled gracefully.
        
        FIX VERIFIED: Persistence failure is caught and logged,
        doesn't break the collection pipeline.
        """
        from backend.services.intel.collectors.quota_manager import QuotaManager, QuotaBucket
        
        # Make Redis fail on set
        mock_redis.set.side_effect = Exception("Redis connection lost")
        
        manager = QuotaManager(mock_redis)
        manager._quota_bucket = QuotaBucket(
            window_start=datetime.now(timezone.utc),
            units_used=100,
        )
        manager._initialized = True
        
        # This should NOT raise anymore - fix implemented
        await manager._persist_quota()
        
        logger.info(
            "AUDIT: Quota persistence failure is now handled gracefully. âœ“ "
            "Failure is logged but doesn't break the pipeline."
        )


# =============================================================================
# SECTION 2: COLLECTOR SILENT FAILURE TESTS
# =============================================================================

class TestCollectorSilentFailures:
    """Tests for batch collector behavior under failure conditions."""
    
    @pytest.fixture
    def mock_youtube(self):
        """Create a mock YouTube client."""
        youtube = AsyncMock()
        youtube.fetch_trending.return_value = []
        youtube.fetch_video_stats.return_value = []
        return youtube
    
    @pytest.fixture
    def mock_quota_manager(self):
        """Create a mock quota manager."""
        quota = MagicMock()
        quota.check_quota.return_value = None
        quota.record_collection = AsyncMock()
        quota.record_failure = AsyncMock()
        return quota
    
    @pytest.fixture
    def mock_redis(self):
        """Create a mock async Redis client."""
        redis = AsyncMock()
        redis.get.return_value = None
        redis.set.return_value = True
        redis.setex.return_value = True
        return redis

    @pytest.mark.asyncio
    async def test_partial_batch_failure_continues(
        self, mock_youtube, mock_quota_manager, mock_redis
    ):
        """
        AUDIT: When some games fail in a batch, others continue.
        Failed games are tracked in errors dict but no exception raised.
        
        RISK: Callers may not notice partial failures.
        """
        from backend.services.intel.collectors.batch_collector import BatchCollector
        
        # Make first game succeed, second fail
        call_count = 0
        async def mock_fetch_trending(**kwargs):
            nonlocal call_count
            call_count += 1
            if call_count == 2:
                raise Exception("API rate limit exceeded")
            # Return video with matching game name in title
            return [{"video_id": f"vid_{call_count}", "title": "Fortnite gameplay", "tags": ["fortnite"]}]
        
        mock_youtube.fetch_trending = mock_fetch_trending
        
        # Create a proper mock for video stats
        class MockVideo:
            def __init__(self, video_id):
                self.video_id = video_id
                self.view_count = 1000
        
        mock_youtube.fetch_video_stats.return_value = [MockVideo("vid_1")]
        
        collector = BatchCollector(mock_youtube, mock_quota_manager, mock_redis)
        collector.MAX_RETRIES = 1  # Speed up test
        collector.RETRY_BASE_DELAY = 0.01
        
        result = await collector.collect_batch(["fortnite", "valorant"])
        
        # AUDIT FINDING: Partial success - one game collected, one failed
        assert len(result.games_collected) == 1, f"Expected 1 game collected, got {result.games_collected}"
        assert len(result.errors) == 1, f"Expected 1 error, got {result.errors}"
        assert "valorant" in result.errors
        
        # No exception raised - caller must check errors dict
        logger.warning(
            f"AUDIT: Batch collection had {len(result.errors)} failures. "
            "Errors are in result.errors dict but no exception raised. "
            "Callers must explicitly check for partial failures."
        )
    
    @pytest.mark.asyncio
    async def test_retry_exhaustion_silent(
        self, mock_youtube, mock_quota_manager, mock_redis
    ):
        """
        AUDIT: After all retries exhausted, failure is recorded but not raised.
        
        RISK: Persistent API issues may go unnoticed.
        """
        from backend.services.intel.collectors.batch_collector import BatchCollector
        
        # Make all retries fail
        mock_youtube.fetch_trending.side_effect = Exception("Persistent API error")
        
        collector = BatchCollector(mock_youtube, mock_quota_manager, mock_redis)
        collector.MAX_RETRIES = 2  # Reduce for faster test
        collector.RETRY_BASE_DELAY = 0.01  # Speed up test
        
        result = await collector.collect_batch(["fortnite"])
        
        # AUDIT FINDING: All retries failed, recorded in errors
        assert len(result.games_collected) == 0
        assert "fortnite" in result.errors
        
        # quota.record_failure should have been called
        mock_quota_manager.record_failure.assert_called()
        
        logger.warning(
            "AUDIT: All retries exhausted for fortnite. "
            "Failure recorded via record_failure() but no exception raised."
        )
    
    @pytest.mark.asyncio
    async def test_video_details_batch_partial_failure(
        self, mock_youtube, mock_quota_manager, mock_redis
    ):
        """
        AUDIT: If video details fetch fails for some batches, 
        we return partial data without indication.
        """
        from backend.services.intel.collectors.batch_collector import BatchCollector
        from backend.services.intel.core.exceptions import IntelQuotaError
        
        # Return videos from trending
        mock_youtube.fetch_trending.return_value = [
            {"video_id": f"vid_{i}", "title": f"Video {i}"}
            for i in range(100)  # 100 videos = 2 batches of 50
        ]
        
        # First batch succeeds, second fails due to quota
        call_count = 0
        async def mock_fetch_stats(video_ids):
            nonlocal call_count
            call_count += 1
            if call_count == 2:
                raise IntelQuotaError(
                    message="Quota exhausted",
                    platform="youtube",
                    units_required=1,
                    units_remaining=0,
                )
            return [
                MagicMock(video_id=vid, __dict__={"video_id": vid, "view_count": 1000})
                for vid in video_ids
            ]
        
        mock_youtube.fetch_video_stats = mock_fetch_stats
        
        collector = BatchCollector(mock_youtube, mock_quota_manager, mock_redis)
        
        result = await collector.collect_batch(["fortnite"])
        
        # AUDIT FINDING: Only first batch of videos returned
        # No indication that second batch was skipped
        assert result.unique_videos < 100
        
        logger.warning(
            f"AUDIT: Only {result.unique_videos}/100 videos fetched due to quota. "
            "Partial video details returned without explicit indication."
        )
    
    @pytest.mark.asyncio
    async def test_content_hash_collision_handling(
        self, mock_youtube, mock_quota_manager, mock_redis
    ):
        """
        AUDIT: Content hash is used for change detection.
        Test that hash collisions don't cause issues.
        """
        from backend.services.intel.collectors.batch_collector import BatchCollector
        
        collector = BatchCollector(mock_youtube, mock_quota_manager, mock_redis)
        
        # Two different video sets that might have similar hashes
        videos1 = [{"video_id": "a", "view_count": 100}]
        videos2 = [{"video_id": "b", "view_count": 100}]
        
        hash1 = collector._calculate_content_hash(videos1)
        hash2 = collector._calculate_content_hash(videos2)
        
        # Hashes should be different for different content
        assert hash1 != hash2, "Different content should have different hashes"
        
        # Same content should have same hash
        hash1_again = collector._calculate_content_hash(videos1)
        assert hash1 == hash1_again, "Same content should have same hash"
        
        logger.info(
            "AUDIT: Content hash correctly differentiates content. âœ“"
        )


# =============================================================================
# SECTION 3: ANALYZER ERROR ISOLATION TESTS
# =============================================================================

class TestAnalyzerErrorIsolation:
    """Tests for analyzer runner error handling and isolation."""
    
    @pytest.mark.asyncio
    async def test_one_analyzer_failure_doesnt_stop_others(self):
        """
        AUDIT: If one analyzer fails, others should still run.
        """
        from backend.services.intel.analyzers.runner import AnalyzerRunner
        
        runner = AnalyzerRunner(max_concurrent=5)
        
        # Mock analyzers - make one fail
        with patch.object(runner, 'ANALYZERS', [
            ("content_format", lambda: self._create_mock_analyzer(succeed=True)),
            ("description", lambda: self._create_mock_analyzer(succeed=False)),
            ("semantic", lambda: self._create_mock_analyzer(succeed=True)),
        ]):
            result = await runner.run_category("fortnite")
        
        # AUDIT FINDING: Failed analyzer doesn't stop others
        assert len(result.analyzers_succeeded) == 2
        assert len(result.analyzers_failed) == 1
        assert "description" in result.analyzers_failed
        
        logger.info(
            "AUDIT: Analyzer failure isolation working correctly. "
            f"{len(result.analyzers_succeeded)} succeeded, "
            f"{len(result.analyzers_failed)} failed. âœ“"
        )
    
    def _create_mock_analyzer(self, succeed: bool):
        """Create a mock analyzer that succeeds or fails."""
        analyzer = MagicMock()
        if succeed:
            mock_result = MagicMock()
            mock_result.to_dict.return_value = {"data": "test"}
            analyzer.analyze_with_cache = AsyncMock(return_value=mock_result)
        else:
            analyzer.analyze_with_cache = AsyncMock(
                side_effect=Exception("Analyzer failed")
            )
        return analyzer

    @pytest.mark.asyncio
    async def test_analyzer_timeout_handling(self):
        """
        AUDIT: Analyzers should have timeout protection.
        Test that slow analyzers don't block the pipeline.
        
        FINDING: The analyzer runner does NOT have built-in per-analyzer timeout.
        This test demonstrates the issue by timing out at the test level.
        """
        from backend.services.intel.analyzers.runner import AnalyzerRunner
        
        runner = AnalyzerRunner(max_concurrent=1)
        
        # Create a slow analyzer
        async def slow_analyze(*args, **kwargs):
            await asyncio.sleep(10)  # Very slow
            return MagicMock(to_dict=lambda: {"data": "test"})
        
        slow_analyzer = MagicMock()
        slow_analyzer.analyze_with_cache = slow_analyze
        
        with patch.object(runner, 'ANALYZERS', [
            ("slow_analyzer", lambda: slow_analyzer),
        ]):
            # Run with a short timeout - this will timeout because
            # the analyzer runner has no built-in timeout
            with pytest.raises(asyncio.TimeoutError):
                await asyncio.wait_for(
                    runner.run_category("fortnite"),
                    timeout=0.5  # Very short timeout
                )
        
        # AUDIT FINDING: No built-in timeout in analyzer runner
        logger.warning(
            "AUDIT: Analyzer runner has no built-in timeout per analyzer. "
            "Slow analyzers can block the entire pipeline. "
            "RECOMMENDATION: Add per-analyzer timeout in run_one()."
        )
    
    @pytest.mark.asyncio
    async def test_analyzer_returns_none_handling(self):
        """
        AUDIT: When analyzer returns None (insufficient data),
        it's recorded as a failure with specific message.
        """
        from backend.services.intel.analyzers.runner import AnalyzerRunner
        
        runner = AnalyzerRunner()
        
        # Analyzer that returns None
        none_analyzer = MagicMock()
        none_analyzer.analyze_with_cache = AsyncMock(return_value=None)
        
        with patch.object(runner, 'ANALYZERS', [
            ("content_format", lambda: none_analyzer),
        ]):
            result = await runner.run_category("fortnite")
        
        # AUDIT FINDING: None result is treated as failure
        assert "content_format" in result.analyzers_failed
        assert "insufficient data" in result.errors.get("content_format", "").lower()
        
        logger.info(
            "AUDIT: None results correctly recorded as failures. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_batch_analyzer_run_sequential(self):
        """
        AUDIT: Batch analyzer runs categories sequentially.
        One category failure doesn't stop others.
        """
        from backend.services.intel.analyzers.runner import AnalyzerRunner
        
        runner = AnalyzerRunner()
        
        # Track which categories were processed
        processed = []
        
        async def track_analyze(category_key, *args, **kwargs):
            processed.append(category_key)
            if category_key == "valorant":
                raise Exception("Valorant analysis failed")
            return MagicMock(to_dict=lambda: {"data": category_key})
        
        mock_analyzer = MagicMock()
        mock_analyzer.analyze_with_cache = track_analyze
        
        with patch.object(runner, 'ANALYZERS', [
            ("test_analyzer", lambda: mock_analyzer),
        ]):
            results = await runner.run_batch(
                ["fortnite", "valorant", "minecraft"]
            )
        
        # All categories should be processed
        assert len(results) == 3
        assert results["fortnite"].analyzers_succeeded == ["test_analyzer"]
        assert results["valorant"].analyzers_failed == ["test_analyzer"]
        assert results["minecraft"].analyzers_succeeded == ["test_analyzer"]
        
        logger.info(
            "AUDIT: Batch analyzer correctly isolates category failures. âœ“"
        )


# =============================================================================
# SECTION 4: AGGREGATION DATA INTEGRITY TESTS
# =============================================================================

class TestAggregationDataIntegrity:
    """Tests for hourly and daily aggregation data integrity."""
    
    @pytest.fixture
    def mock_redis(self):
        """Create a mock async Redis client."""
        redis = AsyncMock()
        redis.get.return_value = None
        return redis
    
    @pytest.fixture
    def mock_supabase(self):
        """Create a mock Supabase client."""
        supabase = MagicMock()
        supabase.table.return_value.upsert.return_value.execute.return_value = MagicMock(data=[])
        supabase.table.return_value.select.return_value.eq.return_value.gte.return_value.order.return_value.execute.return_value = MagicMock(data=[])
        return supabase
    
    @pytest.mark.asyncio
    async def test_hourly_aggregation_missing_redis_data(
        self, mock_redis, mock_supabase
    ):
        """
        AUDIT: When Redis data is missing, category is silently skipped.
        
        RISK: Missing data may go unnoticed in aggregation.
        """
        from backend.services.intel.aggregation.hourly import HourlyAggregator
        
        # Redis returns None for all games
        mock_redis.get.return_value = None
        
        aggregator = HourlyAggregator(mock_redis, mock_supabase)
        
        count = await aggregator.run()
        
        # AUDIT FINDING: All categories skipped, no error raised
        assert count == 0
        
        logger.warning(
            "AUDIT: Hourly aggregation skipped all categories due to missing Redis data. "
            "No error or warning raised - data gaps may go unnoticed."
        )
    
    @pytest.mark.asyncio
    async def test_hourly_aggregation_malformed_json(
        self, mock_redis, mock_supabase
    ):
        """
        AUDIT: Malformed JSON in Redis causes category to be skipped.
        """
        from backend.services.intel.aggregation.hourly import HourlyAggregator
        
        # Return malformed JSON
        mock_redis.get.return_value = "not valid json {"
        
        aggregator = HourlyAggregator(mock_redis, mock_supabase)
        
        count = await aggregator.run()
        
        # AUDIT FINDING: Malformed JSON causes silent skip
        assert count == 0
        
        logger.warning(
            "AUDIT: Malformed JSON in Redis causes silent skip. "
            "Data corruption may go unnoticed."
        )

    @pytest.mark.asyncio
    async def test_hourly_aggregation_db_failure_raises(
        self, mock_redis, mock_supabase
    ):
        """
        AUDIT: Database insertion failure raises exception.
        This is correct behavior - aggregation should fail loudly.
        """
        from backend.services.intel.aggregation.hourly import HourlyAggregator
        
        # Return valid Redis data
        mock_redis.get.return_value = json.dumps({
            "videos": [
                {"video_id": "v1", "view_count": 1000, "engagement_rate": 0.05}
            ]
        })
        
        # Make DB fail
        mock_supabase.table.return_value.upsert.return_value.execute.side_effect = Exception(
            "Database connection failed"
        )
        
        aggregator = HourlyAggregator(mock_redis, mock_supabase)
        
        with pytest.raises(Exception) as exc_info:
            await aggregator.run()
        
        assert "Database connection failed" in str(exc_info.value)
        
        logger.info(
            "AUDIT: Database failures correctly raise exceptions. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_daily_rollup_missing_hourly_data(self, mock_supabase):
        """
        AUDIT: Daily rollup with no hourly data returns None for category.
        """
        from backend.services.intel.aggregation.daily import DailyRollup
        
        # Return empty hourly data
        mock_supabase.table.return_value.select.return_value.eq.return_value.gte.return_value.lt.return_value.order.return_value.execute.return_value = MagicMock(data=[])
        
        rollup = DailyRollup(mock_supabase)
        
        result = await rollup._rollup_game("fortnite", date.today())
        
        # AUDIT FINDING: Missing hourly data returns None
        assert result is None
        
        logger.info(
            "AUDIT: Daily rollup correctly returns None for missing hourly data. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_daily_rollup_trend_calculation_division_by_zero(self, mock_supabase):
        """
        AUDIT: Trend calculation handles division by zero when previous day has 0 values.
        """
        from backend.services.intel.aggregation.daily import DailyRollup
        
        # Return hourly data
        mock_supabase.table.return_value.select.return_value.eq.return_value.gte.return_value.lt.return_value.order.return_value.execute.return_value = MagicMock(data=[
            {
                "category_key": "fortnite",
                "hour_start": datetime.now(timezone.utc).isoformat(),
                "video_count": 10,
                "avg_views": 1000,
                "avg_engagement": 0.05,
                "total_views": 10000,
                "viral_count": 2,
                "rising_count": 1,
                "avg_velocity": 5.0,
                "max_velocity": 10.0,
                "shorts_count": 3,
                "shorts_avg_views": 500,
                "longform_count": 7,
                "longform_avg_views": 1200,
                "avg_duration_seconds": 300,
                "optimal_duration_bucket": "5-10min",
                "language_distribution": {"en": 8, "es": 2},
                "dominant_language": "en",
            }
        ])
        
        # Previous day has 0 values
        mock_supabase.table.return_value.select.return_value.eq.return_value.eq.return_value.single.return_value.execute.return_value = MagicMock(data={
            "avg_views": 0,  # Division by zero risk
            "avg_viral_count": 0,
        })
        
        rollup = DailyRollup(mock_supabase)
        
        # This should NOT raise
        result = await rollup._rollup_game("fortnite", date.today())
        
        # AUDIT FINDING: Division by zero is handled
        assert result is not None
        assert result.views_trend == 0.0  # No trend when previous is 0
        
        logger.info(
            "AUDIT: Trend calculation handles division by zero correctly. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_cleanup_old_data_failure_silent(self, mock_supabase):
        """
        AUDIT: Cleanup failure is logged but doesn't raise.
        
        RISK: Old data may accumulate if cleanup consistently fails.
        """
        from backend.services.intel.aggregation.daily import DailyRollup
        
        # Make cleanup fail
        mock_supabase.table.return_value.delete.return_value.lt.return_value.execute.side_effect = Exception(
            "Cleanup failed"
        )
        
        rollup = DailyRollup(mock_supabase)
        
        # This should NOT raise
        await rollup._cleanup_old_data(date.today())
        
        logger.warning(
            "AUDIT: Cleanup failure is silent. "
            "Old data may accumulate if cleanup consistently fails."
        )


# =============================================================================
# SECTION 5: ORCHESTRATOR COORDINATION TESTS
# =============================================================================

class TestOrchestratorCoordination:
    """Tests for orchestrator task scheduling and coordination."""
    
    @pytest.fixture
    def mock_redis(self):
        """Create a mock async Redis client."""
        redis = AsyncMock()
        redis.get.return_value = None
        redis.set.return_value = True
        redis.ping.return_value = True
        return redis
    
    @pytest.fixture
    def mock_supabase(self):
        """Create a mock Supabase client."""
        supabase = MagicMock()
        supabase.table.return_value.select.return_value.limit.return_value.execute.return_value = MagicMock(data=[])
        return supabase
    
    @pytest.mark.asyncio
    async def test_task_backoff_on_consecutive_failures(
        self, mock_redis, mock_supabase
    ):
        """
        AUDIT: Tasks use exponential backoff after failures.
        """
        from backend.workers.intel.orchestrator import ScheduledTask, TaskPriority
        
        async def failing_handler():
            raise Exception("Task failed")
        
        task = ScheduledTask(
            name="test_task",
            handler=failing_handler,
            interval_seconds=60,
            priority=TaskPriority.NORMAL,
            backoff_multiplier=2.0,
        )
        
        # Simulate failures
        task.consecutive_failures = 3
        task.last_run = datetime.now(timezone.utc)
        
        # Effective interval should be 60 * 2^3 = 480 seconds
        next_run = task.get_next_run()
        expected_interval = 60 * (2 ** 3)  # 480 seconds
        
        actual_interval = (next_run - task.last_run).total_seconds()
        assert abs(actual_interval - expected_interval) < 1
        
        logger.info(
            f"AUDIT: Task backoff working correctly. "
            f"After 3 failures, interval is {actual_interval}s (expected {expected_interval}s). âœ“"
        )

    @pytest.mark.asyncio
    async def test_task_state_persistence_failure(
        self, mock_redis, mock_supabase
    ):
        """
        AUDIT: Task state persistence failure is now handled gracefully.
        
        FIX VERIFIED: Persistence failure is caught and logged,
        doesn't cause task execution to fail.
        """
        from backend.workers.intel.orchestrator import IntelOrchestrator, ScheduledTask, TaskPriority
        
        # Make Redis fail on set
        mock_redis.set.side_effect = Exception("Redis connection lost")
        
        orchestrator = IntelOrchestrator(mock_redis, mock_supabase)
        
        task = ScheduledTask(
            name="test_task",
            handler=AsyncMock(),
            interval_seconds=60,
            priority=TaskPriority.NORMAL,
        )
        task.last_run = datetime.now(timezone.utc)
        
        # This should NOT raise anymore - fix implemented
        await orchestrator._persist_task_state(task)
        
        logger.info(
            "AUDIT: Task state persistence failure is now handled gracefully. âœ“ "
            "Failure is logged but doesn't cause task execution to fail."
        )
    
    @pytest.mark.asyncio
    async def test_task_execution_timeout(self, mock_redis, mock_supabase):
        """
        AUDIT: Tasks have timeout protection.
        """
        from backend.workers.intel.orchestrator import IntelOrchestrator, ScheduledTask, TaskPriority
        
        async def slow_handler():
            await asyncio.sleep(10)  # Very slow
        
        orchestrator = IntelOrchestrator(mock_redis, mock_supabase)
        
        task = ScheduledTask(
            name="slow_task",
            handler=slow_handler,
            interval_seconds=60,
            priority=TaskPriority.NORMAL,
            timeout_seconds=1,  # 1 second timeout
        )
        
        orchestrator.tasks["slow_task"] = task
        
        # Execute task
        await orchestrator._execute_task(task)
        
        # Task should have timed out
        assert task.last_error == "Timeout"
        assert task.consecutive_failures == 1
        
        logger.info(
            "AUDIT: Task timeout protection working correctly. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_concurrent_task_prevention(self, mock_redis, mock_supabase):
        """
        AUDIT: Same task cannot run concurrently.
        """
        from backend.workers.intel.orchestrator import ScheduledTask, TaskPriority
        
        task = ScheduledTask(
            name="test_task",
            handler=AsyncMock(),
            interval_seconds=60,
            priority=TaskPriority.NORMAL,
        )
        
        # Mark as running
        task.is_running = True
        task.last_run = datetime.now(timezone.utc) - timedelta(hours=1)  # Overdue
        
        # Should not run because already running
        assert task.should_run(datetime.now(timezone.utc)) is False
        
        logger.info(
            "AUDIT: Concurrent task prevention working correctly. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_graceful_shutdown_waits_for_tasks(
        self, mock_redis, mock_supabase
    ):
        """
        AUDIT: Graceful shutdown waits for running tasks.
        """
        from backend.workers.intel.orchestrator import IntelOrchestrator, ScheduledTask, TaskPriority
        
        orchestrator = IntelOrchestrator(mock_redis, mock_supabase)
        
        # Create a task that's "running"
        task = ScheduledTask(
            name="running_task",
            handler=AsyncMock(),
            interval_seconds=60,
            priority=TaskPriority.NORMAL,
        )
        task.is_running = True
        orchestrator.tasks["running_task"] = task
        
        # Start shutdown in background
        shutdown_task = asyncio.create_task(orchestrator._graceful_shutdown())
        
        # Wait a bit then mark task as done
        await asyncio.sleep(0.1)
        task.is_running = False
        
        # Shutdown should complete
        await asyncio.wait_for(shutdown_task, timeout=2.0)
        
        logger.info(
            "AUDIT: Graceful shutdown correctly waits for running tasks. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_manual_task_trigger(self, mock_redis, mock_supabase):
        """
        AUDIT: Tasks can be manually triggered.
        """
        from backend.workers.intel.orchestrator import IntelOrchestrator, ScheduledTask, TaskPriority
        
        orchestrator = IntelOrchestrator(mock_redis, mock_supabase)
        orchestrator.register_tasks()
        
        # Set last_run to recent time
        task = orchestrator.tasks["youtube_collection"]
        task.last_run = datetime.now(timezone.utc)
        
        # Task should not be scheduled normally
        assert task.should_run(datetime.now(timezone.utc)) is False
        
        # Manual trigger should work
        result = await orchestrator.trigger_task("youtube_collection")
        assert result is True
        
        # Now task should be scheduled
        assert task.last_run is None  # Reset by trigger
        
        logger.info(
            "AUDIT: Manual task trigger working correctly. âœ“"
        )


# =============================================================================
# SECTION 6: EDGE CASES AND BOUNDARY CONDITIONS
# =============================================================================

class TestEdgeCases:
    """Tests for edge cases and boundary conditions."""
    
    @pytest.mark.asyncio
    async def test_empty_video_list_handling(self):
        """
        AUDIT: Empty video lists are handled gracefully.
        """
        from backend.services.intel.collectors.batch_collector import BatchCollector
        
        mock_youtube = AsyncMock()
        mock_youtube.fetch_trending.return_value = []
        
        mock_quota = MagicMock()
        mock_quota.check_quota.return_value = None
        mock_quota.record_collection = AsyncMock()
        
        mock_redis = AsyncMock()
        mock_redis.get.return_value = None
        mock_redis.setex.return_value = True
        mock_redis.set.return_value = True
        
        collector = BatchCollector(mock_youtube, mock_quota, mock_redis)
        
        result = await collector.collect_batch(["fortnite"])
        
        # Should handle empty results gracefully
        assert result.total_videos == 0
        assert result.unique_videos == 0
        assert len(result.errors) == 0  # Not an error, just no videos
        
        logger.info(
            "AUDIT: Empty video lists handled gracefully. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_language_distribution_empty(self):
        """
        AUDIT: Empty language distribution doesn't cause errors.
        """
        from backend.services.intel.aggregation.daily import DailyRollup
        import math
        
        mock_supabase = MagicMock()
        
        # Return hourly data with empty language distribution
        mock_supabase.table.return_value.select.return_value.eq.return_value.gte.return_value.lt.return_value.order.return_value.execute.return_value = MagicMock(data=[
            {
                "category_key": "fortnite",
                "hour_start": datetime.now(timezone.utc).isoformat(),
                "video_count": 10,
                "avg_views": 1000,
                "avg_engagement": 0.05,
                "total_views": 10000,
                "viral_count": 2,
                "rising_count": 1,
                "avg_velocity": 5.0,
                "max_velocity": 10.0,
                "shorts_count": 3,
                "shorts_avg_views": 500,
                "longform_count": 7,
                "longform_avg_views": 1200,
                "avg_duration_seconds": 300,
                "optimal_duration_bucket": "5-10min",
                "language_distribution": {},  # Empty!
                "dominant_language": "en",
            }
        ])
        
        # No previous day
        mock_supabase.table.return_value.select.return_value.eq.return_value.eq.return_value.single.return_value.execute.side_effect = Exception("Not found")
        
        rollup = DailyRollup(mock_supabase)
        
        result = await rollup._rollup_game("fortnite", date.today())
        
        # Should handle empty language distribution
        assert result is not None
        assert result.dominant_language == "en"  # Default
        assert result.language_diversity_score == 0.0
        
        logger.info(
            "AUDIT: Empty language distribution handled correctly. âœ“"
        )

    @pytest.mark.asyncio
    async def test_very_large_video_count(self):
        """
        AUDIT: Large video counts don't cause memory issues.
        """
        from backend.services.intel.collectors.batch_collector import BatchCollector
        
        mock_youtube = AsyncMock()
        mock_quota = MagicMock()
        mock_quota.check_quota.return_value = None
        mock_quota.record_collection = AsyncMock()
        mock_redis = AsyncMock()
        mock_redis.get.return_value = None
        mock_redis.setex.return_value = True
        mock_redis.set.return_value = True
        
        collector = BatchCollector(mock_youtube, mock_quota, mock_redis)
        
        # Test content hash with large video list
        large_videos = [
            {"video_id": f"vid_{i}", "view_count": i * 100}
            for i in range(10000)
        ]
        
        # Should complete without memory issues
        content_hash = collector._calculate_content_hash(large_videos)
        
        assert len(content_hash) == 16  # SHA256 truncated to 16 chars
        
        logger.info(
            "AUDIT: Large video counts handled correctly. âœ“"
        )
    
    @pytest.mark.asyncio
    async def test_unicode_in_video_titles(self):
        """
        AUDIT: Unicode characters in video data don't cause issues.
        """
        from backend.services.intel.collectors.batch_collector import BatchCollector
        
        mock_youtube = AsyncMock()
        mock_quota = MagicMock()
        mock_redis = AsyncMock()
        
        collector = BatchCollector(mock_youtube, mock_quota, mock_redis)
        
        # Videos with unicode titles
        unicode_videos = [
            {"video_id": "v1", "view_count": 100, "title": "æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«"},
            {"video_id": "v2", "view_count": 200, "title": "Ã‰moji ðŸŽ® Test"},
            {"video_id": "v3", "view_count": 300, "title": "ÐšÐ¸Ñ€Ð¸Ð»Ð»Ð¸Ñ†Ð°"},
        ]
        
        # Should handle unicode without issues
        content_hash = collector._calculate_content_hash(unicode_videos)
        
        assert len(content_hash) == 16
        
        logger.info(
            "AUDIT: Unicode in video data handled correctly. âœ“"
        )


# =============================================================================
# SECTION 7: RECOMMENDATIONS SUMMARY
# =============================================================================

class TestAuditSummary:
    """Summary of audit findings and recommendations."""
    
    def test_print_audit_summary(self):
        """Print comprehensive audit summary."""
        findings = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    CREATOR INTEL V2 AUDIT SUMMARY                            â•‘
â•‘                         âœ… ALL CRITICAL ISSUES FIXED                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  ðŸ”§ FIXES IMPLEMENTED:                                                       â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â•‘
â•‘  1. âœ… Per-analyzer timeout added (default 60s)                              â•‘
â•‘     â†’ AnalyzerRunner now uses asyncio.wait_for per analyzer                  â•‘
â•‘     â†’ Slow analyzers timeout individually without blocking pipeline          â•‘
â•‘                                                                              â•‘
â•‘  2. âœ… Quota exhaustion now visible via get_collection_schedule_detailed()   â•‘
â•‘     â†’ Returns CollectionScheduleResult with scheduled + skipped dict         â•‘
â•‘     â†’ Each skipped game has a reason (quota, not due, circuit breaker)       â•‘
â•‘                                                                              â•‘
â•‘  3. âœ… BatchCollectionResult.raise_if_failed() method added                  â•‘
â•‘     â†’ Callers can enforce minimum success rate                               â•‘
â•‘     â†’ result.raise_if_failed(min_success_rate=0.5) for 50% threshold         â•‘
â•‘     â†’ Also added has_errors, all_failed, success_rate properties             â•‘
â•‘                                                                              â•‘
â•‘  4. âœ… Task state persistence failure now handled gracefully                 â•‘
â•‘     â†’ Wrapped in try/except with logging                                     â•‘
â•‘     â†’ Task execution success not affected by Redis failures                  â•‘
â•‘                                                                              â•‘
â•‘  5. âœ… Quota persistence failure now handled gracefully                      â•‘
â•‘     â†’ Wrapped in try/except with logging                                     â•‘
â•‘     â†’ Collection pipeline not broken by Redis failures                       â•‘
â•‘                                                                              â•‘
â•‘  ðŸŸ¢ WORKING CORRECTLY:                                                       â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â•‘
â•‘  âœ“ Circuit breaker opens after consecutive failures                          â•‘
â•‘  âœ“ Quota reset timezone handling (midnight PT)                               â•‘
â•‘  âœ“ Analyzer failure isolation (one failure doesn't stop others)              â•‘
â•‘  âœ“ Database failures raise exceptions (fail loudly)                          â•‘
â•‘  âœ“ Task timeout protection                                                   â•‘
â•‘  âœ“ Concurrent task prevention                                                â•‘
â•‘  âœ“ Graceful shutdown waits for running tasks                                 â•‘
â•‘  âœ“ Manual task trigger                                                       â•‘
â•‘  âœ“ Exponential backoff on task failures                                      â•‘
â•‘  âœ“ Content hash change detection                                             â•‘
â•‘  âœ“ Empty video list handling                                                 â•‘
â•‘  âœ“ Division by zero protection in trend calculations                         â•‘
â•‘  âœ“ Unicode handling in video data                                            â•‘
â•‘  âœ“ Retry logic with exponential backoff in collectors                        â•‘
â•‘  âœ“ Batch analyzer category isolation                                         â•‘
â•‘                                                                              â•‘
â•‘  ðŸ“Š ARCHITECTURE STRENGTHS:                                                  â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â•‘
â•‘  âœ“ Well-defined exception hierarchy (IntelError subclasses)                  â•‘
â•‘  âœ“ Tiered data storage (Redis â†’ PostgreSQL hourly â†’ daily)                   â•‘
â•‘  âœ“ Quota-aware collection with adaptive scheduling                           â•‘
â•‘  âœ“ Batched API calls to minimize quota usage                                 â•‘
â•‘  âœ“ Content hashing for change detection                                      â•‘
â•‘  âœ“ Priority-based task scheduling                                            â•‘
â•‘  âœ“ Video filtering by game terms (title/tags matching)                       â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(findings)
        logger.info(findings)


# =============================================================================
# RUN CONFIGURATION
# =============================================================================

if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
