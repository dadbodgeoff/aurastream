# ðŸ§  CREATOR INTEL V2 - ENTERPRISE ARCHITECTURE
## Production-Grade Data Pipeline for Real-Time Creator Intelligence

**Version:** 2.0.0  
**Created:** January 2, 2026  
**Author:** Chief Engineer Review  
**Status:** âœ… IMPLEMENTED

---

## IMPLEMENTATION STATUS

| Component | Status | Location |
|-----------|--------|----------|
| Core Infrastructure | âœ… Complete | `backend/services/intel/core/` |
| Quota Manager | âœ… Complete | `backend/services/intel/collectors/quota_manager.py` |
| Batch Collector | âœ… Complete | `backend/services/intel/collectors/batch_collector.py` |
| Content Hasher | âœ… Complete | `backend/services/intel/collectors/content_hasher.py` |
| Content Format Analyzer | âœ… Complete | `backend/services/intel/analyzers/content_format.py` |
| Description Analyzer | âœ… Complete | `backend/services/intel/analyzers/description.py` |
| Semantic Analyzer | âœ… Complete | `backend/services/intel/analyzers/semantic.py` |
| Regional Analyzer | âœ… Complete | `backend/services/intel/analyzers/regional.py` |
| Live Stream Analyzer | âœ… Complete | `backend/services/intel/analyzers/live_stream.py` |
| Analyzer Runner | âœ… Complete | `backend/services/intel/analyzers/runner.py` |
| Hourly Aggregation | âœ… Complete | `backend/services/intel/aggregation/hourly.py` |
| Daily Rollup | âœ… Complete | `backend/services/intel/aggregation/daily.py` |
| API Routes | âœ… Complete | `backend/services/intel/api/routes.py` |
| API Schemas | âœ… Complete | `backend/services/intel/api/schemas.py` |
| Orchestrator | âœ… Complete | `backend/workers/intel/orchestrator.py` |
| Health Checker | âœ… Complete | `backend/workers/intel/health.py` |
| CLI | âœ… Complete | `backend/workers/intel/cli.py` |
| Database Migration | âœ… Complete | `backend/database/migrations/070_intel_v2_schema.sql` |

---

## EXECUTIVE CRITIQUE OF V1

The V1 schema had fundamental flaws:

| Issue | V1 Approach | Problem |
|-------|-------------|---------|
| API Calls | Sequential per game | Wastes quota, slow, no batching |
| Rate Limiting | None | Will hit YouTube 10k/day limit fast |
| Data Aggregation | "Daily snapshot" | No actual rollup strategy |
| Worker Design | Simple loop | No backpressure, no circuit breaker |
| Observability | None | Can't debug production issues |
| Incremental Updates | Recompute all | Wasteful, slow, expensive |

**This V2 redesign addresses all of these with enterprise patterns.**

---

## 1. CORE ARCHITECTURE PRINCIPLES

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ENTERPRISE DESIGN PRINCIPLES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. QUOTA-AWARE COLLECTION                                                  â”‚
â”‚     â””â”€â”€ YouTube: 10,000 units/day budget                                    â”‚
â”‚     â””â”€â”€ Twitch: 800 req/min (generous, but respect it)                      â”‚
â”‚     â””â”€â”€ Adaptive scheduling based on remaining quota                        â”‚
â”‚                                                                             â”‚
â”‚  2. INCREMENTAL PROCESSING                                                  â”‚
â”‚     â””â”€â”€ Only process CHANGED data                                           â”‚
â”‚     â””â”€â”€ Delta detection via content hashing                                 â”‚
â”‚     â””â”€â”€ Skip unchanged categories                                           â”‚
â”‚                                                                             â”‚
â”‚  3. TIERED AGGREGATION                                                      â”‚
â”‚     â””â”€â”€ Hot: Redis (real-time, 4hr TTL)                                     â”‚
â”‚     â””â”€â”€ Warm: PostgreSQL hourly rollups (7 days)                            â”‚
â”‚     â””â”€â”€ Cold: PostgreSQL daily rollups (90 days)                            â”‚
â”‚     â””â”€â”€ Archive: S3/compressed (forever)                                    â”‚
â”‚                                                                             â”‚
â”‚  4. CIRCUIT BREAKER PATTERN                                                 â”‚
â”‚     â””â”€â”€ Fail fast on API errors                                             â”‚
â”‚     â””â”€â”€ Exponential backoff with jitter                                     â”‚
â”‚     â””â”€â”€ Graceful degradation to cached data                                 â”‚
â”‚                                                                             â”‚
â”‚  5. OBSERVABILITY FIRST                                                     â”‚
â”‚     â””â”€â”€ Structured logging with correlation IDs                             â”‚
â”‚     â””â”€â”€ Prometheus metrics for all operations                               â”‚
â”‚     â””â”€â”€ Health endpoints for each worker                                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. QUOTA-AWARE API COLLECTION

### 2.1 YouTube Quota Budget Management

YouTube API costs:
- `videos.list` with chart=mostPopular: **1 unit**
- `videos.list` with id={ids}`: **1 unit per 50 videos**
- `search.list`: **100 units** (AVOID)

**Daily Budget: 10,000 units**

```python
"""
Quota-Aware YouTube Collector

File: backend/services/collectors/youtube_quota_manager.py
"""

import asyncio
import logging
from dataclasses import dataclass, field
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional
import json

logger = logging.getLogger(__name__)


@dataclass
class QuotaBucket:
    """Tracks quota usage for a time window."""
    window_start: datetime
    units_used: int = 0
    units_limit: int = 10000
    
    @property
    def units_remaining(self) -> int:
        return max(0, self.units_limit - self.units_used)
    
    @property
    def is_exhausted(self) -> bool:
        return self.units_remaining <= 0
    
    def consume(self, units: int) -> bool:
        """Consume quota units. Returns False if insufficient."""
        if self.units_remaining < units:
            return False
        self.units_used += units
        return True


@dataclass
class CollectionPriority:
    """Priority configuration for a game category."""
    game_key: str
    priority: int  # 1=highest, 5=lowest
    min_refresh_hours: float  # Minimum time between refreshes
    max_videos: int  # Max videos to fetch per refresh
    
    # Adaptive fields
    last_fetch: Optional[datetime] = None
    last_change_detected: Optional[datetime] = None
    consecutive_no_change: int = 0


class YouTubeQuotaManager:
    """
    Manages YouTube API quota across all collection operations.
    
    Key features:
    - Daily quota tracking with automatic reset at midnight PT
    - Priority-based collection scheduling
    - Adaptive refresh rates based on category activity
    - Circuit breaker for API failures
    """
    
    # Quota costs
    COST_TRENDING = 1
    COST_VIDEO_DETAILS = 1  # Per 50 videos
    COST_SEARCH = 100  # NEVER USE IN PRODUCTION
    
    # Priority defaults
    DEFAULT_PRIORITIES = {
        "fortnite": CollectionPriority("fortnite", 1, 2.0, 50),
        "valorant": CollectionPriority("valorant", 1, 2.0, 50),
        "minecraft": CollectionPriority("minecraft", 1, 2.0, 50),
        "apex_legends": CollectionPriority("apex_legends", 2, 3.0, 40),
        "warzone": CollectionPriority("warzone", 2, 3.0, 40),
        "gta": CollectionPriority("gta", 2, 4.0, 40),
        "roblox": CollectionPriority("roblox", 2, 4.0, 40),
        "league_of_legends": CollectionPriority("league_of_legends", 3, 4.0, 30),
    }
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self._quota_bucket: Optional[QuotaBucket] = None
        self._priorities: Dict[str, CollectionPriority] = {}
        self._circuit_open = False
        self._circuit_open_until: Optional[datetime] = None
    
    async def initialize(self):
        """Load quota state from Redis."""
        # Load or create quota bucket
        quota_data = await self.redis.get("youtube:quota:bucket")
        if quota_data:
            data = json.loads(quota_data)
            window_start = datetime.fromisoformat(data["window_start"])
            
            # Check if we need to reset (new day in PT)
            if self._is_new_quota_day(window_start):
                self._quota_bucket = self._create_new_bucket()
            else:
                self._quota_bucket = QuotaBucket(
                    window_start=window_start,
                    units_used=data["units_used"],
                )
        else:
            self._quota_bucket = self._create_new_bucket()
        
        # Load priorities with last fetch times
        for key, default in self.DEFAULT_PRIORITIES.items():
            priority_data = await self.redis.get(f"youtube:priority:{key}")
            if priority_data:
                data = json.loads(priority_data)
                default.last_fetch = datetime.fromisoformat(data["last_fetch"]) if data.get("last_fetch") else None
                default.last_change_detected = datetime.fromisoformat(data["last_change"]) if data.get("last_change") else None
                default.consecutive_no_change = data.get("no_change_count", 0)
            self._priorities[key] = default
        
        await self._persist_quota()
    
    def _is_new_quota_day(self, window_start: datetime) -> bool:
        """Check if quota should reset (midnight PT)."""
        # YouTube quota resets at midnight Pacific Time
        from zoneinfo import ZoneInfo
        pt = ZoneInfo("America/Los_Angeles")
        
        now_pt = datetime.now(pt)
        window_pt = window_start.astimezone(pt)
        
        return now_pt.date() > window_pt.date()
    
    def _create_new_bucket(self) -> QuotaBucket:
        """Create a fresh quota bucket."""
        return QuotaBucket(
            window_start=datetime.now(timezone.utc),
            units_used=0,
        )
    
    async def _persist_quota(self):
        """Persist quota state to Redis."""
        await self.redis.set(
            "youtube:quota:bucket",
            json.dumps({
                "window_start": self._quota_bucket.window_start.isoformat(),
                "units_used": self._quota_bucket.units_used,
            })
        )
    
    def get_collection_schedule(self) -> List[str]:
        """
        Get ordered list of games to collect based on:
        1. Priority level
        2. Time since last fetch
        3. Remaining quota
        4. Activity level (skip stale categories)
        
        Returns game keys in collection order.
        """
        if self._circuit_open:
            if datetime.now(timezone.utc) < self._circuit_open_until:
                logger.warning("Circuit breaker open, skipping collection")
                return []
            self._circuit_open = False
        
        now = datetime.now(timezone.utc)
        candidates = []
        
        for key, priority in self._priorities.items():
            # Check if enough time has passed
            if priority.last_fetch:
                hours_since = (now - priority.last_fetch).total_seconds() / 3600
                
                # Adaptive: increase interval if no changes detected
                effective_interval = priority.min_refresh_hours
                if priority.consecutive_no_change > 3:
                    effective_interval *= 1.5  # Slow down for stale categories
                if priority.consecutive_no_change > 6:
                    effective_interval *= 2.0
                
                if hours_since < effective_interval:
                    continue
            
            # Estimate quota cost
            estimated_cost = self.COST_TRENDING + (priority.max_videos // 50 + 1)
            
            if self._quota_bucket.units_remaining < estimated_cost:
                logger.warning(f"Insufficient quota for {key}, skipping")
                continue
            
            # Score: lower is better (priority * hours_overdue)
            hours_overdue = 0
            if priority.last_fetch:
                hours_overdue = max(0, (now - priority.last_fetch).total_seconds() / 3600 - priority.min_refresh_hours)
            else:
                hours_overdue = 24  # Never fetched = high priority
            
            score = priority.priority - (hours_overdue * 0.5)
            candidates.append((key, score))
        
        # Sort by score (lower = higher priority)
        candidates.sort(key=lambda x: x[1])
        
        return [key for key, _ in candidates]
    
    async def record_collection(
        self,
        game_key: str,
        units_used: int,
        videos_fetched: int,
        content_hash: str,
        previous_hash: Optional[str],
    ):
        """Record a collection operation."""
        now = datetime.now(timezone.utc)
        
        # Update quota
        self._quota_bucket.consume(units_used)
        await self._persist_quota()
        
        # Update priority
        priority = self._priorities.get(game_key)
        if priority:
            priority.last_fetch = now
            
            # Check if content changed
            if previous_hash and content_hash == previous_hash:
                priority.consecutive_no_change += 1
            else:
                priority.consecutive_no_change = 0
                priority.last_change_detected = now
            
            # Persist
            await self.redis.set(
                f"youtube:priority:{game_key}",
                json.dumps({
                    "last_fetch": priority.last_fetch.isoformat(),
                    "last_change": priority.last_change_detected.isoformat() if priority.last_change_detected else None,
                    "no_change_count": priority.consecutive_no_change,
                })
            )
        
        logger.info(
            f"Collected {game_key}: {videos_fetched} videos, "
            f"{units_used} quota units, "
            f"remaining: {self._quota_bucket.units_remaining}"
        )
    
    async def record_failure(self, game_key: str, error: Exception):
        """Record an API failure and potentially open circuit breaker."""
        logger.error(f"Collection failed for {game_key}: {error}")
        
        # Track consecutive failures
        failures_key = f"youtube:failures:{game_key}"
        failures = await self.redis.incr(failures_key)
        await self.redis.expire(failures_key, 3600)  # Reset after 1 hour
        
        if failures >= 3:
            # Open circuit breaker
            self._circuit_open = True
            self._circuit_open_until = datetime.now(timezone.utc) + timedelta(minutes=15)
            logger.warning(f"Circuit breaker opened until {self._circuit_open_until}")
    
    def get_quota_status(self) -> Dict:
        """Get current quota status for monitoring."""
        return {
            "units_used": self._quota_bucket.units_used,
            "units_remaining": self._quota_bucket.units_remaining,
            "units_limit": self._quota_bucket.units_limit,
            "percent_used": round(self._quota_bucket.units_used / self._quota_bucket.units_limit * 100, 1),
            "window_start": self._quota_bucket.window_start.isoformat(),
            "circuit_open": self._circuit_open,
        }
```


### 2.2 Batched Collection Strategy

```python
"""
Batched Video Collection

Instead of fetching videos one game at a time, batch operations:
1. Fetch trending for ALL games in parallel (respecting rate limits)
2. Dedupe video IDs across games
3. Batch video details requests (50 IDs per request)

File: backend/services/collectors/batch_collector.py
"""

import asyncio
import hashlib
from typing import Dict, List, Set, Tuple
from dataclasses import dataclass


@dataclass
class BatchCollectionResult:
    """Result of a batch collection operation."""
    games_collected: List[str]
    total_videos: int
    unique_videos: int
    quota_used: int
    duration_seconds: float
    content_hashes: Dict[str, str]  # game_key -> hash


class BatchCollector:
    """
    Collects YouTube data for multiple games efficiently.
    
    Strategy:
    1. Parallel trending fetch (1 unit each, run concurrently)
    2. Collect all video IDs, dedupe
    3. Single batched details fetch (1 unit per 50 videos)
    4. Distribute results back to games
    """
    
    MAX_CONCURRENT_TRENDING = 4  # Don't hammer API
    BATCH_SIZE = 50  # YouTube max per request
    
    def __init__(self, youtube_client, quota_manager, redis_client):
        self.youtube = youtube_client
        self.quota = quota_manager
        self.redis = redis_client
    
    async def collect_batch(self, game_keys: List[str]) -> BatchCollectionResult:
        """
        Collect data for multiple games in optimized batch.
        """
        import time
        start = time.time()
        
        # Phase 1: Parallel trending fetch
        trending_results = await self._fetch_trending_parallel(game_keys)
        
        # Phase 2: Collect and dedupe video IDs
        all_video_ids: Set[str] = set()
        game_video_map: Dict[str, List[str]] = {}
        
        for game_key, videos in trending_results.items():
            video_ids = [v["video_id"] for v in videos]
            game_video_map[game_key] = video_ids
            all_video_ids.update(video_ids)
        
        # Phase 3: Batch fetch video details
        video_details = await self._fetch_details_batched(list(all_video_ids))
        
        # Phase 4: Distribute and store
        content_hashes = {}
        for game_key, video_ids in game_video_map.items():
            game_videos = [
                video_details[vid] for vid in video_ids 
                if vid in video_details
            ]
            
            # Calculate content hash for change detection
            content_hash = self._calculate_content_hash(game_videos)
            previous_hash = await self.redis.get(f"youtube:hash:{game_key}")
            
            # Store in Redis
            await self._store_game_videos(game_key, game_videos)
            content_hashes[game_key] = content_hash
            
            # Record collection
            await self.quota.record_collection(
                game_key=game_key,
                units_used=0,  # Tracked separately
                videos_fetched=len(game_videos),
                content_hash=content_hash,
                previous_hash=previous_hash,
            )
        
        # Calculate quota used
        trending_cost = len(game_keys)  # 1 unit each
        details_cost = (len(all_video_ids) + self.BATCH_SIZE - 1) // self.BATCH_SIZE
        total_quota = trending_cost + details_cost
        
        return BatchCollectionResult(
            games_collected=list(game_keys),
            total_videos=sum(len(ids) for ids in game_video_map.values()),
            unique_videos=len(all_video_ids),
            quota_used=total_quota,
            duration_seconds=time.time() - start,
            content_hashes=content_hashes,
        )
    
    async def _fetch_trending_parallel(
        self, 
        game_keys: List[str]
    ) -> Dict[str, List[Dict]]:
        """Fetch trending videos for multiple games in parallel."""
        semaphore = asyncio.Semaphore(self.MAX_CONCURRENT_TRENDING)
        
        async def fetch_one(game_key: str) -> Tuple[str, List[Dict]]:
            async with semaphore:
                videos = await self.youtube.fetch_trending(
                    category="gaming",
                    max_results=50,
                )
                # Filter to game-specific (by title/tags matching)
                # This is a simplification - real impl would use search
                return game_key, videos
        
        tasks = [fetch_one(key) for key in game_keys]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        output = {}
        for result in results:
            if isinstance(result, Exception):
                continue
            game_key, videos = result
            output[game_key] = videos
        
        return output
    
    async def _fetch_details_batched(
        self, 
        video_ids: List[str]
    ) -> Dict[str, Dict]:
        """Fetch video details in batches of 50."""
        all_details = {}
        
        for i in range(0, len(video_ids), self.BATCH_SIZE):
            batch = video_ids[i:i + self.BATCH_SIZE]
            details = await self.youtube.fetch_video_stats(batch)
            
            for video in details:
                all_details[video.video_id] = video.__dict__
            
            # Small delay between batches
            if i + self.BATCH_SIZE < len(video_ids):
                await asyncio.sleep(0.1)
        
        return all_details
    
    def _calculate_content_hash(self, videos: List[Dict]) -> str:
        """Calculate hash of video content for change detection."""
        # Hash based on video IDs and view counts
        content = "|".join(
            f"{v.get('video_id')}:{v.get('view_count')}"
            for v in sorted(videos, key=lambda x: x.get('video_id', ''))
        )
        return hashlib.sha256(content.encode()).hexdigest()[:16]
    
    async def _store_game_videos(self, game_key: str, videos: List[Dict]):
        """Store videos in Redis with metadata."""
        import json
        from datetime import datetime, timezone
        
        data = {
            "game_key": game_key,
            "videos": videos,
            "video_count": len(videos),
            "fetched_at": datetime.now(timezone.utc).isoformat(),
        }
        
        await self.redis.setex(
            f"youtube:games:{game_key}",
            18000,  # 5 hour TTL
            json.dumps(data),
        )
        
        # Store hash for change detection
        content_hash = self._calculate_content_hash(videos)
        await self.redis.set(f"youtube:hash:{game_key}", content_hash)
```


---

## 3. TIERED AGGREGATION PIPELINE

### 3.1 The Three-Tier Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TIERED DATA AGGREGATION                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  HOT TIER (Redis)                                                           â”‚
â”‚  â”œâ”€â”€ Purpose: Real-time serving, sub-millisecond reads                      â”‚
â”‚  â”œâ”€â”€ TTL: 4-6 hours                                                         â”‚
â”‚  â”œâ”€â”€ Data: Raw videos, pre-computed analysis                                â”‚
â”‚  â””â”€â”€ Keys: youtube:games:{game}, intel:*:precomputed:{game}                 â”‚
â”‚                                                                             â”‚
â”‚  WARM TIER (PostgreSQL - Hourly)                                            â”‚
â”‚  â”œâ”€â”€ Purpose: Recent history, trend detection                               â”‚
â”‚  â”œâ”€â”€ Retention: 7 days                                                      â”‚
â”‚  â”œâ”€â”€ Data: Hourly aggregates (avg views, viral count, etc.)                 â”‚
â”‚  â””â”€â”€ Tables: intel_hourly_*                                                 â”‚
â”‚                                                                             â”‚
â”‚  COLD TIER (PostgreSQL - Daily)                                             â”‚
â”‚  â”œâ”€â”€ Purpose: Historical analysis, long-term trends                         â”‚
â”‚  â”œâ”€â”€ Retention: 90 days                                                     â”‚
â”‚  â”œâ”€â”€ Data: Daily rollups from hourly data                                   â”‚
â”‚  â””â”€â”€ Tables: intel_daily_*                                                  â”‚
â”‚                                                                             â”‚
â”‚  ARCHIVE TIER (S3 - Monthly)                                                â”‚
â”‚  â”œâ”€â”€ Purpose: Compliance, deep historical analysis                          â”‚
â”‚  â”œâ”€â”€ Retention: Forever                                                     â”‚
â”‚  â”œâ”€â”€ Data: Compressed monthly exports                                       â”‚
â”‚  â””â”€â”€ Format: Parquet files                                                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Aggregation Worker

```python
"""
Tiered Aggregation Worker

Handles rollup from Hot â†’ Warm â†’ Cold tiers.

Schedule:
- Hourly rollup: Every hour at :05
- Daily rollup: Every day at 00:15 UTC
- Monthly archive: 1st of month at 01:00 UTC

File: backend/workers/intel_aggregation_worker.py
"""

import asyncio
import logging
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)


@dataclass
class HourlyAggregate:
    """Hourly aggregated metrics for a category."""
    category_key: str
    hour_start: datetime
    
    # Video metrics
    video_count: int
    avg_views: float
    avg_engagement: float
    total_views: int
    
    # Viral metrics
    viral_count: int
    rising_count: int
    avg_velocity: float
    max_velocity: float
    
    # Format metrics
    shorts_count: int
    shorts_avg_views: float
    longform_count: int
    longform_avg_views: float
    
    # Duration metrics
    avg_duration_seconds: float
    optimal_duration_bucket: str
    
    # Regional metrics
    language_distribution: Dict[str, int]
    dominant_language: str


@dataclass  
class DailyAggregate:
    """Daily aggregated metrics rolled up from hourly."""
    category_key: str
    date: datetime
    
    # Aggregated from hourly
    total_videos_seen: int
    peak_viral_count: int
    avg_viral_count: float
    
    # Trend indicators
    views_trend: float  # % change from previous day
    viral_trend: float
    
    # Best performing
    best_hour_utc: int
    worst_hour_utc: int
    
    # Format insights
    shorts_performance_ratio: float
    optimal_duration_range: str


class AggregationWorker:
    """
    Manages tiered data aggregation.
    
    Responsibilities:
    1. Hourly: Aggregate Redis data â†’ PostgreSQL hourly tables
    2. Daily: Roll up hourly â†’ daily tables
    3. Monthly: Archive daily â†’ S3
    4. Cleanup: Prune old data per retention policy
    """
    
    HOURLY_RETENTION_DAYS = 7
    DAILY_RETENTION_DAYS = 90
    
    def __init__(self, redis_client, db_pool):
        self.redis = redis_client
        self.db = db_pool
    
    async def run_hourly_aggregation(self):
        """
        Aggregate current Redis data into hourly PostgreSQL records.
        
        Called every hour at :05.
        """
        hour_start = datetime.now(timezone.utc).replace(
            minute=0, second=0, microsecond=0
        )
        
        logger.info(f"Starting hourly aggregation for {hour_start}")
        
        # Get all game keys
        game_keys = await self._get_tracked_games()
        
        aggregates = []
        for game_key in game_keys:
            try:
                aggregate = await self._aggregate_game_hourly(game_key, hour_start)
                if aggregate:
                    aggregates.append(aggregate)
            except Exception as e:
                logger.error(f"Failed to aggregate {game_key}: {e}")
        
        # Batch insert to PostgreSQL
        await self._insert_hourly_aggregates(aggregates)
        
        logger.info(f"Hourly aggregation complete: {len(aggregates)} categories")
    
    async def run_daily_rollup(self):
        """
        Roll up yesterday's hourly data into daily aggregate.
        
        Called daily at 00:15 UTC.
        """
        yesterday = (datetime.now(timezone.utc) - timedelta(days=1)).date()
        
        logger.info(f"Starting daily rollup for {yesterday}")
        
        game_keys = await self._get_tracked_games()
        
        rollups = []
        for game_key in game_keys:
            try:
                rollup = await self._rollup_game_daily(game_key, yesterday)
                if rollup:
                    rollups.append(rollup)
            except Exception as e:
                logger.error(f"Failed to rollup {game_key}: {e}")
        
        await self._insert_daily_rollups(rollups)
        
        # Cleanup old hourly data
        await self._cleanup_old_hourly(yesterday - timedelta(days=self.HOURLY_RETENTION_DAYS))
        
        logger.info(f"Daily rollup complete: {len(rollups)} categories")
    
    async def _aggregate_game_hourly(
        self, 
        game_key: str, 
        hour_start: datetime
    ) -> Optional[HourlyAggregate]:
        """Aggregate a single game's data for the hour."""
        # Load from Redis
        youtube_data = await self.redis.get(f"youtube:games:{game_key}")
        viral_data = await self.redis.get(f"intel:viral:precomputed:{game_key}")
        format_data = await self.redis.get(f"intel:format:precomputed:{game_key}")
        
        if not youtube_data:
            return None
        
        videos = json.loads(youtube_data).get("videos", [])
        viral = json.loads(viral_data) if viral_data else {}
        format_intel = json.loads(format_data) if format_data else {}
        
        # Calculate aggregates
        views = [v.get("view_count", 0) for v in videos]
        engagements = [v.get("engagement_rate", 0) or 0 for v in videos]
        durations = [v.get("duration_seconds", 0) for v in videos if v.get("duration_seconds")]
        
        shorts = [v for v in videos if v.get("is_short")]
        longform = [v for v in videos if not v.get("is_short")]
        
        # Language distribution
        lang_dist = {}
        for v in videos:
            lang = (v.get("default_audio_language") or "en")[:2].lower()
            lang_dist[lang] = lang_dist.get(lang, 0) + 1
        
        dominant_lang = max(lang_dist.items(), key=lambda x: x[1])[0] if lang_dist else "en"
        
        return HourlyAggregate(
            category_key=game_key,
            hour_start=hour_start,
            video_count=len(videos),
            avg_views=sum(views) / len(views) if views else 0,
            avg_engagement=sum(engagements) / len(engagements) if engagements else 0,
            total_views=sum(views),
            viral_count=viral.get("viral_video_count", 0),
            rising_count=viral.get("rising_video_count", 0),
            avg_velocity=viral.get("avg_velocity", 0),
            max_velocity=viral.get("max_velocity", 0),
            shorts_count=len(shorts),
            shorts_avg_views=sum(v.get("view_count", 0) for v in shorts) / len(shorts) if shorts else 0,
            longform_count=len(longform),
            longform_avg_views=sum(v.get("view_count", 0) for v in longform) / len(longform) if longform else 0,
            avg_duration_seconds=sum(durations) / len(durations) if durations else 0,
            optimal_duration_bucket=format_intel.get("optimal_duration_range", "unknown"),
            language_distribution=lang_dist,
            dominant_language=dominant_lang,
        )
    
    async def _rollup_game_daily(
        self, 
        game_key: str, 
        date: datetime
    ) -> Optional[DailyAggregate]:
        """Roll up hourly data into daily aggregate."""
        # Query hourly data for the day
        async with self.db.acquire() as conn:
            rows = await conn.fetch("""
                SELECT * FROM intel_hourly_metrics
                WHERE category_key = $1 
                AND hour_start >= $2 
                AND hour_start < $3
                ORDER BY hour_start
            """, game_key, date, date + timedelta(days=1))
        
        if not rows:
            return None
        
        # Calculate daily aggregates
        total_videos = sum(r["video_count"] for r in rows)
        viral_counts = [r["viral_count"] for r in rows]
        
        # Find best/worst hours
        by_views = sorted(rows, key=lambda r: r["avg_views"], reverse=True)
        best_hour = by_views[0]["hour_start"].hour if by_views else 12
        worst_hour = by_views[-1]["hour_start"].hour if by_views else 4
        
        # Calculate trends (compare to previous day)
        prev_day = await self._get_previous_daily(game_key, date - timedelta(days=1))
        
        current_avg_views = sum(r["avg_views"] for r in rows) / len(rows)
        views_trend = 0
        if prev_day and prev_day.get("avg_views", 0) > 0:
            views_trend = (current_avg_views - prev_day["avg_views"]) / prev_day["avg_views"] * 100
        
        # Shorts performance
        shorts_views = sum(r["shorts_avg_views"] * r["shorts_count"] for r in rows)
        shorts_count = sum(r["shorts_count"] for r in rows)
        longform_views = sum(r["longform_avg_views"] * r["longform_count"] for r in rows)
        longform_count = sum(r["longform_count"] for r in rows)
        
        shorts_ratio = 1.0
        if longform_count > 0 and shorts_count > 0:
            shorts_avg = shorts_views / shorts_count
            longform_avg = longform_views / longform_count
            shorts_ratio = shorts_avg / longform_avg if longform_avg > 0 else 1.0
        
        return DailyAggregate(
            category_key=game_key,
            date=date,
            total_videos_seen=total_videos,
            peak_viral_count=max(viral_counts) if viral_counts else 0,
            avg_viral_count=sum(viral_counts) / len(viral_counts) if viral_counts else 0,
            views_trend=round(views_trend, 2),
            viral_trend=0,  # Calculate similarly
            best_hour_utc=best_hour,
            worst_hour_utc=worst_hour,
            shorts_performance_ratio=round(shorts_ratio, 2),
            optimal_duration_range=rows[0]["optimal_duration_bucket"] if rows else "unknown",
        )
    
    async def _insert_hourly_aggregates(self, aggregates: List[HourlyAggregate]):
        """Batch insert hourly aggregates."""
        if not aggregates:
            return
        
        async with self.db.acquire() as conn:
            await conn.executemany("""
                INSERT INTO intel_hourly_metrics (
                    category_key, hour_start, video_count, avg_views, 
                    avg_engagement, total_views, viral_count, rising_count,
                    avg_velocity, max_velocity, shorts_count, shorts_avg_views,
                    longform_count, longform_avg_views, avg_duration_seconds,
                    optimal_duration_bucket, language_distribution, dominant_language
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18)
                ON CONFLICT (category_key, hour_start) DO UPDATE SET
                    video_count = EXCLUDED.video_count,
                    avg_views = EXCLUDED.avg_views,
                    viral_count = EXCLUDED.viral_count
            """, [(
                a.category_key, a.hour_start, a.video_count, a.avg_views,
                a.avg_engagement, a.total_views, a.viral_count, a.rising_count,
                a.avg_velocity, a.max_velocity, a.shorts_count, a.shorts_avg_views,
                a.longform_count, a.longform_avg_views, a.avg_duration_seconds,
                a.optimal_duration_bucket, json.dumps(a.language_distribution), a.dominant_language
            ) for a in aggregates])
    
    async def _get_tracked_games(self) -> List[str]:
        """Get list of tracked game keys."""
        return [
            "fortnite", "valorant", "minecraft", "apex_legends",
            "warzone", "gta", "roblox", "league_of_legends"
        ]
    
    async def _get_previous_daily(self, game_key: str, date: datetime) -> Optional[Dict]:
        """Get previous day's aggregate for trend calculation."""
        async with self.db.acquire() as conn:
            row = await conn.fetchrow("""
                SELECT * FROM intel_daily_metrics
                WHERE category_key = $1 AND date = $2
            """, game_key, date)
        return dict(row) if row else None
    
    async def _cleanup_old_hourly(self, before_date: datetime):
        """Delete hourly data older than retention period."""
        async with self.db.acquire() as conn:
            result = await conn.execute("""
                DELETE FROM intel_hourly_metrics
                WHERE hour_start < $1
            """, before_date)
            logger.info(f"Cleaned up hourly data before {before_date}: {result}")
```


---

## 4. UNIFIED WORKER ORCHESTRATOR

### 4.1 The Problem with Multiple Workers

V1 had separate workers that could:
- Race each other for resources
- Have inconsistent schedules
- Lack coordination
- No centralized health monitoring

### 4.2 Unified Orchestrator Design

```python
"""
Intel Worker Orchestrator

Single entry point for all intel data operations.
Coordinates collection, analysis, and aggregation.

File: backend/workers/intel_orchestrator.py
"""

import asyncio
import logging
import signal
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass, field
from enum import Enum
import json

logger = logging.getLogger(__name__)


class TaskPriority(Enum):
    CRITICAL = 1  # Must run on schedule
    HIGH = 2      # Important but can slip
    NORMAL = 3    # Best effort
    LOW = 4       # Run if resources available


@dataclass
class ScheduledTask:
    """A task scheduled to run at specific intervals."""
    name: str
    handler: Callable
    interval_seconds: int
    priority: TaskPriority
    
    # Runtime state
    last_run: Optional[datetime] = None
    last_success: Optional[datetime] = None
    last_error: Optional[str] = None
    consecutive_failures: int = 0
    is_running: bool = False
    
    # Configuration
    max_retries: int = 3
    timeout_seconds: int = 300
    backoff_multiplier: float = 2.0
    
    def should_run(self, now: datetime) -> bool:
        """Check if task should run based on schedule."""
        if self.is_running:
            return False
        
        if self.last_run is None:
            return True
        
        # Apply backoff if failing
        effective_interval = self.interval_seconds
        if self.consecutive_failures > 0:
            effective_interval *= (self.backoff_multiplier ** min(self.consecutive_failures, 4))
        
        next_run = self.last_run + timedelta(seconds=effective_interval)
        return now >= next_run
    
    def get_next_run(self) -> Optional[datetime]:
        """Get next scheduled run time."""
        if self.last_run is None:
            return datetime.now(timezone.utc)
        
        effective_interval = self.interval_seconds
        if self.consecutive_failures > 0:
            effective_interval *= (self.backoff_multiplier ** min(self.consecutive_failures, 4))
        
        return self.last_run + timedelta(seconds=effective_interval)


@dataclass
class OrchestratorMetrics:
    """Metrics for monitoring."""
    tasks_executed: int = 0
    tasks_succeeded: int = 0
    tasks_failed: int = 0
    total_duration_seconds: float = 0
    last_health_check: Optional[datetime] = None
    
    def to_dict(self) -> Dict:
        return {
            "tasks_executed": self.tasks_executed,
            "tasks_succeeded": self.tasks_succeeded,
            "tasks_failed": self.tasks_failed,
            "success_rate": self.tasks_succeeded / self.tasks_executed if self.tasks_executed > 0 else 0,
            "avg_duration": self.total_duration_seconds / self.tasks_executed if self.tasks_executed > 0 else 0,
            "last_health_check": self.last_health_check.isoformat() if self.last_health_check else None,
        }


class IntelOrchestrator:
    """
    Unified orchestrator for all intel operations.
    
    Responsibilities:
    1. Schedule and coordinate all data tasks
    2. Manage resource contention
    3. Handle failures with backoff
    4. Provide health/metrics endpoints
    5. Graceful shutdown
    
    Tasks managed:
    - YouTube collection (every 4 hours)
    - Twitch collection (every 30 minutes)
    - Intel analysis (every 4 hours, after YouTube)
    - Hourly aggregation (every hour at :05)
    - Daily rollup (daily at 00:15)
    """
    
    def __init__(
        self,
        redis_client,
        db_pool,
        youtube_collector,
        twitch_collector,
        quota_manager,
    ):
        self.redis = redis_client
        self.db = db_pool
        self.youtube = youtube_collector
        self.twitch = twitch_collector
        self.quota = quota_manager
        
        self.tasks: Dict[str, ScheduledTask] = {}
        self.metrics = OrchestratorMetrics()
        self._shutdown_event = asyncio.Event()
        self._running = False
    
    def register_tasks(self):
        """Register all scheduled tasks."""
        # Collection tasks
        self.tasks["youtube_collection"] = ScheduledTask(
            name="youtube_collection",
            handler=self._run_youtube_collection,
            interval_seconds=4 * 3600,  # 4 hours
            priority=TaskPriority.CRITICAL,
            timeout_seconds=600,
        )
        
        self.tasks["twitch_collection"] = ScheduledTask(
            name="twitch_collection",
            handler=self._run_twitch_collection,
            interval_seconds=30 * 60,  # 30 minutes
            priority=TaskPriority.HIGH,
            timeout_seconds=300,
        )
        
        # Analysis tasks (run after collection)
        self.tasks["intel_analysis"] = ScheduledTask(
            name="intel_analysis",
            handler=self._run_intel_analysis,
            interval_seconds=4 * 3600,  # 4 hours
            priority=TaskPriority.HIGH,
            timeout_seconds=900,
        )
        
        # Aggregation tasks
        self.tasks["hourly_aggregation"] = ScheduledTask(
            name="hourly_aggregation",
            handler=self._run_hourly_aggregation,
            interval_seconds=3600,  # 1 hour
            priority=TaskPriority.NORMAL,
            timeout_seconds=300,
        )
        
        self.tasks["daily_rollup"] = ScheduledTask(
            name="daily_rollup",
            handler=self._run_daily_rollup,
            interval_seconds=24 * 3600,  # 24 hours
            priority=TaskPriority.NORMAL,
            timeout_seconds=600,
        )
        
        # Health check
        self.tasks["health_check"] = ScheduledTask(
            name="health_check",
            handler=self._run_health_check,
            interval_seconds=60,  # 1 minute
            priority=TaskPriority.LOW,
            timeout_seconds=30,
        )
    
    async def start(self):
        """Start the orchestrator main loop."""
        self._running = True
        self.register_tasks()
        
        # Setup signal handlers
        loop = asyncio.get_event_loop()
        for sig in (signal.SIGTERM, signal.SIGINT):
            loop.add_signal_handler(sig, self._handle_shutdown)
        
        logger.info("Intel Orchestrator starting...")
        logger.info(f"Registered {len(self.tasks)} tasks")
        
        # Initialize quota manager
        await self.quota.initialize()
        
        # Main loop
        while not self._shutdown_event.is_set():
            await self._tick()
            await asyncio.sleep(10)  # Check every 10 seconds
        
        logger.info("Intel Orchestrator shutting down...")
        await self._graceful_shutdown()
    
    async def _tick(self):
        """Single tick of the orchestrator loop."""
        now = datetime.now(timezone.utc)
        
        # Get tasks that should run, sorted by priority
        runnable = [
            task for task in self.tasks.values()
            if task.should_run(now)
        ]
        runnable.sort(key=lambda t: t.priority.value)
        
        # Run tasks (one at a time to avoid resource contention)
        for task in runnable:
            if self._shutdown_event.is_set():
                break
            
            await self._execute_task(task)
    
    async def _execute_task(self, task: ScheduledTask):
        """Execute a single task with error handling."""
        import time
        
        task.is_running = True
        task.last_run = datetime.now(timezone.utc)
        start_time = time.time()
        
        logger.info(f"Starting task: {task.name}")
        
        try:
            # Run with timeout
            await asyncio.wait_for(
                task.handler(),
                timeout=task.timeout_seconds
            )
            
            # Success
            task.last_success = datetime.now(timezone.utc)
            task.last_error = None
            task.consecutive_failures = 0
            
            self.metrics.tasks_succeeded += 1
            logger.info(f"Task completed: {task.name}")
            
        except asyncio.TimeoutError:
            task.last_error = "Timeout"
            task.consecutive_failures += 1
            self.metrics.tasks_failed += 1
            logger.error(f"Task timed out: {task.name}")
            
        except Exception as e:
            task.last_error = str(e)
            task.consecutive_failures += 1
            self.metrics.tasks_failed += 1
            logger.error(f"Task failed: {task.name} - {e}")
        
        finally:
            task.is_running = False
            duration = time.time() - start_time
            self.metrics.tasks_executed += 1
            self.metrics.total_duration_seconds += duration
            
            # Persist task state
            await self._persist_task_state(task)
    
    async def _persist_task_state(self, task: ScheduledTask):
        """Persist task state to Redis for recovery."""
        state = {
            "last_run": task.last_run.isoformat() if task.last_run else None,
            "last_success": task.last_success.isoformat() if task.last_success else None,
            "last_error": task.last_error,
            "consecutive_failures": task.consecutive_failures,
        }
        await self.redis.set(
            f"orchestrator:task:{task.name}",
            json.dumps(state)
        )
    
    def _handle_shutdown(self):
        """Handle shutdown signal."""
        logger.info("Shutdown signal received")
        self._shutdown_event.set()
    
    async def _graceful_shutdown(self):
        """Wait for running tasks to complete."""
        running = [t for t in self.tasks.values() if t.is_running]
        if running:
            logger.info(f"Waiting for {len(running)} tasks to complete...")
            for _ in range(30):  # Wait up to 30 seconds
                running = [t for t in self.tasks.values() if t.is_running]
                if not running:
                    break
                await asyncio.sleep(1)
        
        logger.info("Orchestrator shutdown complete")
    
    # Task handlers
    async def _run_youtube_collection(self):
        """Run YouTube data collection."""
        from backend.services.collectors.batch_collector import BatchCollector
        
        collector = BatchCollector(self.youtube, self.quota, self.redis)
        schedule = self.quota.get_collection_schedule()
        
        if not schedule:
            logger.info("No games scheduled for collection")
            return
        
        result = await collector.collect_batch(schedule)
        logger.info(
            f"YouTube collection complete: {result.games_collected}, "
            f"{result.unique_videos} videos, {result.quota_used} quota"
        )
    
    async def _run_twitch_collection(self):
        """Run Twitch data collection."""
        # Implementation similar to YouTube but for Twitch
        pass
    
    async def _run_intel_analysis(self):
        """Run all intel analyzers."""
        from backend.services.intel.analyzer_runner import run_all_analyzers
        
        games = self.quota.get_collection_schedule()
        results = await run_all_analyzers(games, self.redis)
        
        logger.info(f"Intel analysis complete: {len(results)} categories")
    
    async def _run_hourly_aggregation(self):
        """Run hourly aggregation."""
        from backend.workers.intel_aggregation_worker import AggregationWorker
        
        worker = AggregationWorker(self.redis, self.db)
        await worker.run_hourly_aggregation()
    
    async def _run_daily_rollup(self):
        """Run daily rollup."""
        from backend.workers.intel_aggregation_worker import AggregationWorker
        
        worker = AggregationWorker(self.redis, self.db)
        await worker.run_daily_rollup()
    
    async def _run_health_check(self):
        """Run health check and update metrics."""
        self.metrics.last_health_check = datetime.now(timezone.utc)
        
        # Check Redis connectivity
        await self.redis.ping()
        
        # Check DB connectivity
        async with self.db.acquire() as conn:
            await conn.fetchval("SELECT 1")
        
        # Persist metrics
        await self.redis.set(
            "orchestrator:metrics",
            json.dumps(self.metrics.to_dict())
        )
    
    def get_status(self) -> Dict:
        """Get orchestrator status for API."""
        return {
            "running": self._running,
            "metrics": self.metrics.to_dict(),
            "tasks": {
                name: {
                    "last_run": task.last_run.isoformat() if task.last_run else None,
                    "last_success": task.last_success.isoformat() if task.last_success else None,
                    "last_error": task.last_error,
                    "consecutive_failures": task.consecutive_failures,
                    "is_running": task.is_running,
                    "next_run": task.get_next_run().isoformat() if task.get_next_run() else None,
                }
                for name, task in self.tasks.items()
            },
            "quota": self.quota.get_quota_status(),
        }
```


---

## 5. DATABASE SCHEMA (PRODUCTION-GRADE)

### 5.1 Migration File

```sql
-- Migration: 070_intel_v2_schema.sql
-- Enterprise-grade schema for Creator Intel V2

-- ============================================================================
-- HOURLY METRICS (Warm Tier - 7 day retention)
-- ============================================================================

CREATE TABLE IF NOT EXISTS intel_hourly_metrics (
    id BIGSERIAL PRIMARY KEY,
    category_key TEXT NOT NULL,
    hour_start TIMESTAMPTZ NOT NULL,
    
    -- Video metrics
    video_count INTEGER NOT NULL DEFAULT 0,
    avg_views FLOAT NOT NULL DEFAULT 0,
    avg_engagement FLOAT NOT NULL DEFAULT 0,
    total_views BIGINT NOT NULL DEFAULT 0,
    
    -- Viral metrics
    viral_count INTEGER NOT NULL DEFAULT 0,
    rising_count INTEGER NOT NULL DEFAULT 0,
    avg_velocity FLOAT NOT NULL DEFAULT 0,
    max_velocity FLOAT NOT NULL DEFAULT 0,
    
    -- Format metrics
    shorts_count INTEGER NOT NULL DEFAULT 0,
    shorts_avg_views FLOAT NOT NULL DEFAULT 0,
    longform_count INTEGER NOT NULL DEFAULT 0,
    longform_avg_views FLOAT NOT NULL DEFAULT 0,
    
    -- Duration metrics
    avg_duration_seconds FLOAT NOT NULL DEFAULT 0,
    optimal_duration_bucket TEXT,
    
    -- Regional metrics
    language_distribution JSONB NOT NULL DEFAULT '{}',
    dominant_language TEXT NOT NULL DEFAULT 'en',
    
    -- Metadata
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    CONSTRAINT intel_hourly_unique UNIQUE (category_key, hour_start)
);

-- Partition by month for efficient cleanup
-- (In production, use native partitioning)
CREATE INDEX idx_intel_hourly_category_time 
    ON intel_hourly_metrics (category_key, hour_start DESC);

CREATE INDEX idx_intel_hourly_time 
    ON intel_hourly_metrics (hour_start DESC);


-- ============================================================================
-- DAILY METRICS (Cold Tier - 90 day retention)
-- ============================================================================

CREATE TABLE IF NOT EXISTS intel_daily_metrics (
    id BIGSERIAL PRIMARY KEY,
    category_key TEXT NOT NULL,
    date DATE NOT NULL,
    
    -- Aggregated metrics
    total_videos_seen INTEGER NOT NULL DEFAULT 0,
    peak_viral_count INTEGER NOT NULL DEFAULT 0,
    avg_viral_count FLOAT NOT NULL DEFAULT 0,
    
    -- Trend indicators (% change from previous day)
    views_trend FLOAT NOT NULL DEFAULT 0,
    viral_trend FLOAT NOT NULL DEFAULT 0,
    engagement_trend FLOAT NOT NULL DEFAULT 0,
    
    -- Time analysis
    best_hour_utc INTEGER,
    worst_hour_utc INTEGER,
    peak_views_hour_utc INTEGER,
    
    -- Format insights
    shorts_performance_ratio FLOAT NOT NULL DEFAULT 1.0,
    optimal_duration_range TEXT,
    
    -- Regional insights
    dominant_language TEXT NOT NULL DEFAULT 'en',
    language_diversity_score FLOAT NOT NULL DEFAULT 0,
    
    -- Metadata
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    CONSTRAINT intel_daily_unique UNIQUE (category_key, date)
);

CREATE INDEX idx_intel_daily_category_date 
    ON intel_daily_metrics (category_key, date DESC);

CREATE INDEX idx_intel_daily_date 
    ON intel_daily_metrics (date DESC);


-- ============================================================================
-- DURATION PERFORMANCE (Detailed duration analysis)
-- ============================================================================

CREATE TABLE IF NOT EXISTS intel_duration_performance (
    id BIGSERIAL PRIMARY KEY,
    category_key TEXT NOT NULL,
    date DATE NOT NULL,
    
    -- Duration bucket
    bucket_label TEXT NOT NULL,
    bucket_min_seconds INTEGER NOT NULL,
    bucket_max_seconds INTEGER NOT NULL,
    
    -- Performance metrics
    video_count INTEGER NOT NULL DEFAULT 0,
    avg_views FLOAT NOT NULL DEFAULT 0,
    avg_engagement FLOAT NOT NULL DEFAULT 0,
    total_views BIGINT NOT NULL DEFAULT 0,
    performance_index FLOAT NOT NULL DEFAULT 1.0,
    
    -- Metadata
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    CONSTRAINT intel_duration_unique UNIQUE (category_key, date, bucket_label)
);

CREATE INDEX idx_intel_duration_category_date 
    ON intel_duration_performance (category_key, date DESC);


-- ============================================================================
-- REGIONAL PERFORMANCE (Language/region analysis)
-- ============================================================================

CREATE TABLE IF NOT EXISTS intel_regional_performance (
    id BIGSERIAL PRIMARY KEY,
    category_key TEXT NOT NULL,
    date DATE NOT NULL,
    language_code TEXT NOT NULL,
    
    -- YouTube metrics
    youtube_video_count INTEGER NOT NULL DEFAULT 0,
    youtube_avg_views FLOAT NOT NULL DEFAULT 0,
    youtube_total_views BIGINT NOT NULL DEFAULT 0,
    
    -- Twitch metrics
    twitch_stream_count INTEGER NOT NULL DEFAULT 0,
    twitch_avg_viewers FLOAT NOT NULL DEFAULT 0,
    twitch_total_viewers BIGINT NOT NULL DEFAULT 0,
    
    -- Derived scores
    competition_score FLOAT NOT NULL DEFAULT 50,
    opportunity_score FLOAT NOT NULL DEFAULT 50,
    market_share_percent FLOAT NOT NULL DEFAULT 0,
    
    -- Metadata
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    CONSTRAINT intel_regional_unique UNIQUE (category_key, date, language_code)
);

CREATE INDEX idx_intel_regional_category_date 
    ON intel_regional_performance (category_key, date DESC);


-- ============================================================================
-- PREMIERE PERFORMANCE (Live stream timing analysis)
-- ============================================================================

CREATE TABLE IF NOT EXISTS intel_premiere_performance (
    id BIGSERIAL PRIMARY KEY,
    category_key TEXT NOT NULL,
    date DATE NOT NULL,
    
    -- Premiere vs instant
    premiere_count INTEGER NOT NULL DEFAULT 0,
    instant_count INTEGER NOT NULL DEFAULT 0,
    premiere_avg_views FLOAT NOT NULL DEFAULT 0,
    instant_avg_views FLOAT NOT NULL DEFAULT 0,
    performance_ratio FLOAT NOT NULL DEFAULT 1.0,
    
    -- Best times (JSONB array of {hour_utc, day_of_week, performance_index})
    best_times JSONB NOT NULL DEFAULT '[]',
    worst_times JSONB NOT NULL DEFAULT '[]',
    
    -- Schedule adherence
    avg_delay_seconds FLOAT NOT NULL DEFAULT 0,
    on_time_percent FLOAT NOT NULL DEFAULT 0,
    
    -- Metadata
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    CONSTRAINT intel_premiere_unique UNIQUE (category_key, date)
);

CREATE INDEX idx_intel_premiere_category_date 
    ON intel_premiere_performance (category_key, date DESC);


-- ============================================================================
-- WORKER STATE (Orchestrator persistence)
-- ============================================================================

CREATE TABLE IF NOT EXISTS intel_worker_state (
    id SERIAL PRIMARY KEY,
    task_name TEXT NOT NULL UNIQUE,
    last_run TIMESTAMPTZ,
    last_success TIMESTAMPTZ,
    last_error TEXT,
    consecutive_failures INTEGER NOT NULL DEFAULT 0,
    metadata JSONB NOT NULL DEFAULT '{}',
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);


-- ============================================================================
-- QUOTA TRACKING (API quota management)
-- ============================================================================

CREATE TABLE IF NOT EXISTS intel_quota_log (
    id BIGSERIAL PRIMARY KEY,
    platform TEXT NOT NULL,  -- 'youtube', 'twitch'
    operation TEXT NOT NULL,
    units_used INTEGER NOT NULL,
    units_remaining INTEGER NOT NULL,
    category_key TEXT,
    metadata JSONB NOT NULL DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_intel_quota_platform_time 
    ON intel_quota_log (platform, created_at DESC);


-- ============================================================================
-- FUNCTIONS
-- ============================================================================

-- Function to get trending categories
CREATE OR REPLACE FUNCTION get_trending_categories(
    p_days INTEGER DEFAULT 7
)
RETURNS TABLE (
    category_key TEXT,
    avg_viral_count FLOAT,
    views_trend FLOAT,
    opportunity_score FLOAT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        d.category_key,
        AVG(d.avg_viral_count) as avg_viral_count,
        AVG(d.views_trend) as views_trend,
        AVG(d.avg_viral_count * 10 + GREATEST(d.views_trend, 0)) as opportunity_score
    FROM intel_daily_metrics d
    WHERE d.date >= CURRENT_DATE - p_days
    GROUP BY d.category_key
    ORDER BY opportunity_score DESC;
END;
$$ LANGUAGE plpgsql;


-- Function to get optimal duration for a category
CREATE OR REPLACE FUNCTION get_optimal_duration(
    p_category_key TEXT,
    p_days INTEGER DEFAULT 7
)
RETURNS TABLE (
    bucket_label TEXT,
    avg_performance_index FLOAT,
    total_videos INTEGER
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        dp.bucket_label,
        AVG(dp.performance_index) as avg_performance_index,
        SUM(dp.video_count)::INTEGER as total_videos
    FROM intel_duration_performance dp
    WHERE dp.category_key = p_category_key
    AND dp.date >= CURRENT_DATE - p_days
    GROUP BY dp.bucket_label
    HAVING SUM(dp.video_count) >= 5
    ORDER BY avg_performance_index DESC;
END;
$$ LANGUAGE plpgsql;


-- Function to cleanup old data
CREATE OR REPLACE FUNCTION cleanup_intel_data()
RETURNS void AS $$
BEGIN
    -- Cleanup hourly (7 days)
    DELETE FROM intel_hourly_metrics 
    WHERE hour_start < NOW() - INTERVAL '7 days';
    
    -- Cleanup daily (90 days)
    DELETE FROM intel_daily_metrics 
    WHERE date < CURRENT_DATE - 90;
    
    -- Cleanup duration (90 days)
    DELETE FROM intel_duration_performance 
    WHERE date < CURRENT_DATE - 90;
    
    -- Cleanup regional (90 days)
    DELETE FROM intel_regional_performance 
    WHERE date < CURRENT_DATE - 90;
    
    -- Cleanup premiere (90 days)
    DELETE FROM intel_premiere_performance 
    WHERE date < CURRENT_DATE - 90;
    
    -- Cleanup quota log (30 days)
    DELETE FROM intel_quota_log 
    WHERE created_at < NOW() - INTERVAL '30 days';
END;
$$ LANGUAGE plpgsql;
```


---

## 6. DIRECTORY STRUCTURE

```
backend/
â”œâ”€â”€ services/
â”‚   â””â”€â”€ intel/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ core/                          # Core infrastructure
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base_analyzer.py           # Abstract base for all analyzers
â”‚       â”‚   â”œâ”€â”€ decay_manager.py           # Data freshness/confidence
â”‚       â”‚   â””â”€â”€ metrics.py                 # Prometheus metrics
â”‚       â”‚
â”‚       â”œâ”€â”€ collectors/                    # Data collection
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ youtube_quota_manager.py   # Quota tracking
â”‚       â”‚   â”œâ”€â”€ batch_collector.py         # Batched collection
â”‚       â”‚   â”œâ”€â”€ youtube_collector.py       # YouTube API client
â”‚       â”‚   â””â”€â”€ twitch_collector.py        # Twitch API client
â”‚       â”‚
â”‚       â”œâ”€â”€ analyzers/                     # Analysis modules
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ viral_detector.py          # Viral content detection
â”‚       â”‚   â”œâ”€â”€ title_intel.py             # Title/keyword analysis
â”‚       â”‚   â”œâ”€â”€ content_format.py          # Duration/format analysis
â”‚       â”‚   â”œâ”€â”€ description.py             # Description mining
â”‚       â”‚   â”œâ”€â”€ semantic.py                # Topic/tag analysis
â”‚       â”‚   â”œâ”€â”€ regional.py                # Language/region analysis
â”‚       â”‚   â”œâ”€â”€ live_stream.py             # Premiere/scheduling
â”‚       â”‚   â””â”€â”€ competition.py             # Competition analysis
â”‚       â”‚
â”‚       â”œâ”€â”€ aggregation/                   # Data aggregation
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ hourly_aggregator.py       # Redis â†’ PostgreSQL hourly
â”‚       â”‚   â”œâ”€â”€ daily_rollup.py            # Hourly â†’ Daily rollup
â”‚       â”‚   â””â”€â”€ archive_exporter.py        # Daily â†’ S3 archive
â”‚       â”‚
â”‚       â””â”€â”€ api/                           # API layer
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ schemas.py                 # Pydantic schemas
â”‚           â””â”€â”€ routes.py                  # FastAPI routes
â”‚
â”œâ”€â”€ workers/
â”‚   â””â”€â”€ intel/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ orchestrator.py                # Main orchestrator
â”‚       â”œâ”€â”€ health.py                      # Health check endpoints
â”‚       â””â”€â”€ cli.py                         # CLI entry point
â”‚
â””â”€â”€ database/
    â””â”€â”€ migrations/
        â””â”€â”€ 070_intel_v2_schema.sql        # Database schema
```

---

## 7. IMPLEMENTATION CHECKLIST

### Phase 1: Core Infrastructure (Week 1)
- [ ] `core/base_analyzer.py` - Abstract base class
- [ ] `core/decay_manager.py` - Freshness scoring
- [ ] `core/metrics.py` - Prometheus integration
- [ ] `collectors/youtube_quota_manager.py` - Quota tracking
- [ ] `collectors/batch_collector.py` - Batched collection

### Phase 2: Collectors (Week 2)
- [ ] Update `collectors/youtube_collector.py` - Add liveStreamingDetails
- [ ] Update `collectors/twitch_collector.py` - Add /videos, /channels
- [ ] Integration tests for collectors

### Phase 3: Analyzers (Week 3-4)
- [ ] `analyzers/content_format.py`
- [ ] `analyzers/description.py`
- [ ] `analyzers/semantic.py`
- [ ] `analyzers/regional.py`
- [ ] `analyzers/live_stream.py`
- [ ] Unit tests for each analyzer

### Phase 4: Aggregation (Week 5)
- [ ] `aggregation/hourly_aggregator.py`
- [ ] `aggregation/daily_rollup.py`
- [ ] Database migration
- [ ] Integration tests

### Phase 5: Orchestrator (Week 6)
- [ ] `workers/intel/orchestrator.py`
- [ ] `workers/intel/health.py`
- [ ] `workers/intel/cli.py`
- [ ] End-to-end tests

### Phase 6: API & Frontend (Week 7)
- [ ] `api/schemas.py`
- [ ] `api/routes.py`
- [ ] Frontend components
- [ ] Documentation

---

## 8. SUCCESS METRICS

| Metric | Target | Measurement |
|--------|--------|-------------|
| API Quota Efficiency | <50% daily usage | Quota manager logs |
| Data Freshness | <4 hours for all categories | Decay manager |
| Analysis Coverage | 100% of 45 fields | Field usage audit |
| Worker Uptime | >99.9% | Health checks |
| Query Latency | <100ms p95 | API metrics |
| Historical Data | 90 days retention | Database size |

---

## 9. NOVEL ASPECTS

What makes this architecture enterprise-grade:

1. **Quota-Aware Collection** - Adaptive scheduling based on remaining API quota
2. **Content Hashing** - Skip unchanged categories to save resources
3. **Tiered Aggregation** - Hot/Warm/Cold data tiers with automatic rollup
4. **Circuit Breaker** - Fail fast and recover gracefully
5. **Unified Orchestrator** - Single coordination point for all tasks
6. **Backpressure Handling** - Slow down on failures, speed up on success
7. **Observable by Default** - Metrics, health checks, structured logging

---

*This V2 architecture is production-ready and addresses all the shortcomings of V1.*
